{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNu1Y6jmqOoxI/zHcKK8tdg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/parpsyche/eauctionsindia_scrape/blob/main/scrape_delhi_properties.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5DRwjZpf3Q53",
        "outputId": "6df991cc-72b8-4064-98d2-acd19348b92e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Scraping Summary Pages: 100%|██████████| 174/174 [02:23<00:00,  1.21it/s]\n",
            "Scraping Details:  23%|██▎       | 474/2087 [01:14<03:49,  7.04it/s]"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "import logging\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from tqdm import tqdm\n",
        "\n",
        "# --- Configuration ---\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "BASE_URL = \"https://www.eauctionsindia.com\"\n",
        "START_URL = \"https://www.eauctionsindia.com/city/new-delhi/1\"\n",
        "MAX_WORKERS = 10  # Number of threads to use for scraping detail pages\n",
        "HEADERS = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "}\n",
        "\n",
        "# --- Helper Functions for Safe Extraction ---\n",
        "def find_detail_by_strong_text(soup, card_title, detail_label):\n",
        "    \"\"\"A helper to find data points based on their <strong> label within a specific card.\"\"\"\n",
        "    try:\n",
        "        card_header = soup.find('h5', class_='text-secondary', string=card_title)\n",
        "        if not card_header: return 'N/A'\n",
        "\n",
        "        card_body = card_header.find_parent('div', class_='card-header').find_next_sibling('div', class_='card-body')\n",
        "        if not card_body: return 'N/A'\n",
        "\n",
        "        strong_tag = card_body.find('strong', string=lambda text: text and detail_label in text)\n",
        "        if not strong_tag: return 'N/A'\n",
        "\n",
        "        value_tag = strong_tag.find_next_sibling()\n",
        "        return value_tag.get_text(strip=True) if value_tag else 'N/A'\n",
        "    except (AttributeError, TypeError):\n",
        "        return 'N/A'\n",
        "\n",
        "# --- Stage 1: Scraping Listing Pages ---\n",
        "def get_last_page_number(session, url):\n",
        "    \"\"\"Finds the last page number from the pagination section.\"\"\"\n",
        "    try:\n",
        "        logging.info(f\"Determining total number of pages from: {url}\")\n",
        "        response = session.get(url, headers=HEADERS, timeout=15)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        page_numbers = [int(a.text) for a in soup.select('ul.pagination li a.page-link') if a.text.isdigit()]\n",
        "        if not page_numbers:\n",
        "            logging.warning(\"Pagination controls not found. Assuming only one page.\")\n",
        "            return 1\n",
        "\n",
        "        last_page = max(page_numbers)\n",
        "        logging.info(f\"Found {last_page} pages to scrape.\")\n",
        "        return last_page\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Could not determine the last page number: {e}\")\n",
        "        return 0\n",
        "\n",
        "def scrape_summary_page(session, page_url):\n",
        "    \"\"\"Scrapes summary auction listings from a single page.\"\"\"\n",
        "    try:\n",
        "        response = session.get(page_url, headers=HEADERS, timeout=15)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        listing_cards = soup.select('div.col-xl-9 > div.row.mb-3[style*=\"border\"]')\n",
        "        page_auctions = []\n",
        "\n",
        "        for card in listing_cards:\n",
        "            summary_data = {}\n",
        "            title_element = card.find('h5', class_='font-weight-bold')\n",
        "            if title_element:\n",
        "                summary_data['title'] = title_element.get_text(strip=True)\n",
        "                summary_data['property_url'] = BASE_URL + title_element.parent['href']\n",
        "\n",
        "            price_span = card.find(lambda tag: tag.name == 'span' and 'Reserve Price' in tag.text)\n",
        "            summary_data['reserve_price'] = price_span.get_text(strip=True).replace('Reserve Price :', '').strip() if price_span else 'N/A'\n",
        "\n",
        "            auction_id_span = card.find(lambda tag: tag.name == 'span' and 'Auction ID' in tag.text)\n",
        "            summary_data['auction_id'] = auction_id_span.get_text(strip=True).replace('Auction ID :', '').replace('#', '').strip() if auction_id_span else 'N/A'\n",
        "\n",
        "            page_auctions.append(summary_data)\n",
        "        return page_auctions\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        logging.warning(f\"Could not retrieve summary page {page_url}. Error: {e}\")\n",
        "        return []\n",
        "\n",
        "# --- Stage 2: Scraping Detail Pages (Multithreaded) ---\n",
        "def scrape_property_details(args):\n",
        "    \"\"\"Worker function for threads to scrape a single property's detail page.\"\"\"\n",
        "    session, summary_data = args\n",
        "    url = summary_data.get('property_url')\n",
        "\n",
        "    if not url or url == 'N/A':\n",
        "        return summary_data\n",
        "\n",
        "    try:\n",
        "        response = session.get(url, headers=HEADERS, timeout=20)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # --- Extract details using helper function ---\n",
        "        summary_data['emd'] = find_detail_by_strong_text(soup, 'Bank Details', 'EMD :')\n",
        "        summary_data['bank_name'] = find_detail_by_strong_text(soup, 'Bank Details', 'Bank Name :')\n",
        "        summary_data['branch_name_detail'] = find_detail_by_strong_text(soup, 'Bank Details', 'Branch Name :')\n",
        "        summary_data['service_provider'] = find_detail_by_strong_text(soup, 'Bank Details', 'Service Provider :')\n",
        "        summary_data['borrower_name'] = find_detail_by_strong_text(soup, 'Property Details', 'Borrower Name :')\n",
        "        summary_data['property_type_detail'] = find_detail_by_strong_text(soup, 'Property Details', 'Property Type :')\n",
        "        summary_data['auction_start_date'] = find_detail_by_strong_text(soup, 'Property Details', 'Auction Start Date :')\n",
        "        summary_data['auction_end_time'] = find_detail_by_strong_text(soup, 'Property Details', 'Auction End Time :')\n",
        "        summary_data['submission_date'] = find_detail_by_strong_text(soup, 'Property Details', 'Application Subbmision Date :')\n",
        "\n",
        "        # --- CORRECTED: Extract description with a more specific selector ---\n",
        "        try:\n",
        "            desc_header = soup.find('h5', class_='text-secondary', string='Description')\n",
        "            card_body = desc_header.find_parent('.card-header').find_next_sibling('.card-body')\n",
        "            description_p = card_body.find('div', class_='mb-4').find('p')\n",
        "            summary_data['description'] = description_p.get_text(strip=True) if description_p else 'N/A'\n",
        "        except (AttributeError, TypeError):\n",
        "            summary_data['description'] = 'N/A'\n",
        "\n",
        "        # --- CORRECTED: Extract all download links more robustly ---\n",
        "        try:\n",
        "            dl_header = soup.find('h5', class_='text-secondary', string='Downloads')\n",
        "            card_body = dl_header.find_parent('.card-header').find_next_sibling('.card-body')\n",
        "            all_links = []\n",
        "            link_tags = card_body.select('a[href]')\n",
        "            for link in link_tags:\n",
        "                href = link['href']\n",
        "                if href.startswith('/'):\n",
        "                    full_url = BASE_URL + href\n",
        "                else:\n",
        "                    full_url = href\n",
        "                all_links.append(full_url)\n",
        "            summary_data['download_links'] = ', '.join(all_links) if all_links else 'N/A'\n",
        "        except (AttributeError, TypeError):\n",
        "            summary_data['download_links'] = 'N/A'\n",
        "\n",
        "        return summary_data\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        logging.warning(f\"Failed to fetch details for {url}: {e}\")\n",
        "        return summary_data\n",
        "\n",
        "# --- Main Orchestrator ---\n",
        "def main():\n",
        "    with requests.Session() as session:\n",
        "        # --- STAGE 1: Get all summary listings ---\n",
        "        pagination_base_url = START_URL.rsplit('/', 1)[0]\n",
        "        last_page = get_last_page_number(session, START_URL)\n",
        "        if last_page == 0:\n",
        "            logging.error(\"Could not determine pages. Aborting.\")\n",
        "            return\n",
        "\n",
        "        all_summaries = []\n",
        "        logging.info(\"--- STAGE 1: Fetching all property summaries ---\")\n",
        "        for page_num in tqdm(range(1, last_page + 1), desc=\"Scraping Summary Pages\"):\n",
        "            page_url = f\"{pagination_base_url}/{page_num}\"\n",
        "            summaries = scrape_summary_page(session, page_url)\n",
        "            if summaries:\n",
        "                all_summaries.extend(summaries)\n",
        "            time.sleep(0.5)\n",
        "\n",
        "        if not all_summaries:\n",
        "            logging.error(\"No property summaries found. Exiting.\")\n",
        "            return\n",
        "\n",
        "        logging.info(f\"Found {len(all_summaries)} total properties to process.\")\n",
        "\n",
        "        # --- STAGE 2: Use ThreadPoolExecutor to scrape details in parallel ---\n",
        "        logging.info(\"\\n--- STAGE 2: Fetching property details (multithreaded) ---\")\n",
        "        all_detailed_data = []\n",
        "        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
        "            args_for_map = [(session, summary) for summary in all_summaries]\n",
        "\n",
        "            future_to_summary = {executor.submit(scrape_property_details, arg): arg for arg in args_for_map}\n",
        "            for future in tqdm(as_completed(future_to_summary), total=len(all_summaries), desc=\"Scraping Details\"):\n",
        "                try:\n",
        "                    result = future.result()\n",
        "                    if result:\n",
        "                        all_detailed_data.append(result)\n",
        "                except Exception as e:\n",
        "                    logging.error(f\"A thread generated an exception: {e}\")\n",
        "\n",
        "    # --- STAGE 3: Process and save the collected data ---\n",
        "    if not all_detailed_data:\n",
        "        logging.warning(\"Scraping finished, but no detailed data was collected.\")\n",
        "        return\n",
        "\n",
        "    logging.info(f\"Successfully scraped details for {len(all_detailed_data)} listings.\")\n",
        "    df = pd.DataFrame(all_detailed_data)\n",
        "\n",
        "    desired_order = [\n",
        "        'auction_id', 'title', 'reserve_price', 'emd', 'bank_name',\n",
        "        'property_type_detail', 'borrower_name', 'description', 'auction_start_date',\n",
        "        'auction_end_time', 'submission_date', 'branch_name_detail', 'service_provider',\n",
        "        'property_url', 'download_links'\n",
        "    ]\n",
        "    for col in desired_order:\n",
        "        if col not in df.columns:\n",
        "            df[col] = 'N/A'\n",
        "    df = df[desired_order]\n",
        "\n",
        "    print(\"\\n--- Scraping Complete ---\")\n",
        "    print(f\"Total listings processed: {len(df)}\")\n",
        "    print(\"\\nSample of final data:\")\n",
        "    print(df.head())\n",
        "\n",
        "    try:\n",
        "        output_filename = 'eauctionsindia_new-delhi_full_details.csv'\n",
        "        df.to_csv(output_filename, index=False, encoding='utf-8-sig')\n",
        "        logging.info(f\"All data has been successfully saved to {output_filename}\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Failed to save data to CSV file: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}